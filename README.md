==========================================\nIntroduction\n==========================================\nThis repository is managed by ThreadK and is dedicated to reproducing and enhancing research on Learning Activation Functions to Improve Deep Neural Networks. This is a learnable activation function for neural networks, which tests show outperforms traditional rectified linear units (ReLU) and Leaky ReLU.\n\n==========================================\nMemory Usage\n==========================================\nPlease note that this layer may require more memory. Although there is a "save_mem" option, it could lead to slower performance and has not been extensively tested.\n\n==========================================\nIn-place Computation\n==========================================\nIn-place computations can be performed but keep in mind that they do not conserve memory and tests have shown a slight decrease in speed.\n\n==========================================\nSolver Files\n==========================================\nWe have made custom modifications to the solver files. These changes are reflected in src/caff