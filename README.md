==========================================\nIntroduction\n==========================================\nThis repository is managed by ThreadK and is dedicated to reproducing and enhancing research on Learning Activation Functions to Improve Deep Neural Networks. This is a learnable activation function for neural networks, which tests show outperforms traditional rectified linear units (ReLU) and Leaky ReLU.\n\n==========================================\nMemory Usage\n==========================================\nPlease note that this layer may require more memory. Although there is a "save_mem" option, it could lead to slower performance and h